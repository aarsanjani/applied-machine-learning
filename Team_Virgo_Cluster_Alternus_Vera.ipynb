{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team Virgo Cluster - Alternus Vera.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarsanjani/applied-machine-learning/blob/master/Team_Virgo_Cluster_Alternus_Vera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "myrJOvEVIhue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Team Virgo Cluster\n",
        "### Contribution\n",
        "- Sy Le (006088940) : data import and wrangling, tokenization, remove stopwords, remove stemmings\n",
        "- Mojdeh Keykhanzadeh(008129589) : remove punctuation , apply ngrams,researched about IBM Faireness\n",
        "- Hyunwook Shin (012507417) : Build word2vec model for converting raw tokens to structured data, Initial List of Factors \n",
        "- Lin Cheng (012484459) : Use TF-IDF Vectorizer + SVD to produce a matrix, and apply Logistic Regression on it\n",
        "- Yu Xu (012502048): Explored ways to gather topics from the dataset. Explored tf-idf ranking. Explored pipeline + GridSearch for the best n_component of LDA for logistic regression"
      ]
    },
    {
      "metadata": {
        "id": "ebnN4ikZzQtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework: Alternus Vera\n",
        "\n",
        "Due Tuesday by 11:59pm\n",
        "\n",
        "Available Oct 30 at 12am - Nov 13 at 11:59pm 15 days\n",
        "\n",
        "- 1 Read the Article on Fake News (Links to an external site.)Links to an external site. I posted on slack during week 8 lecture. (see 2d below) https://arxiv.org/pdf/1708.01967.pdf\n",
        "- 2 Project Alternus Vera:\n",
        "- 2a Use the Kaggle Fake News Data Set,\n",
        "- 2b Use the \"Liar, Liar\" DataSet from Politifact.com , that I gave access via Google Drive: https://drive.google.com/open?id=1y3yYF5HHPhH7SyaPwPU9H5HApJzevsFK (Links to an external site.)Links to an external site.\n",
        "- 2c Use a data source for enriching your previous project with textual data, such as news, reviews, comments, twitter feeds, etc.\n",
        "- 2d Define factors in a polynomial equation for factors constituting \"Fake News/ Factual News\": e.g., Reliable Source, Political Affiliation, Sensationalism, Echo Chamber, (see other factors you can derive from paper 1. above)\n",
        "- 2e Create 3 classification models that assess at least three of these factors and create corresponding weights for your polynomial equation for coming up with a Fake News score.\n",
        "- 2f Conduct LDA on the three DataSets above and visualize. Interpret results in a Data Narrative; e.g. what are the topics in those data sets?\n",
        "- 2g Conduct some form of regression analysis (I will leave this open for discussion and your innovation)\n",
        "\n",
        "Interpret results in a Data Narrative.\n",
        "\n",
        "## Examples:\n",
        "- 2g1 Given news can we predict what a politician will say?\n",
        "- 2g2 Given a news story, break it into its constituent fake/alternative parts\n",
        "\n",
        "Diagram for Project: https://drive.google.com/open?id=19Xi4zFMMgiCKiKNiqIj_uE_OYEwWoAxQ (Links to an external site.)Links to an external site.\n",
        "\n",
        "Alternative View: https://drive.google.com/open?id=10UdEYplQj5UYf0X0l_i9eYHY0xoIy8nv (Links to an external site.)Links to an external site.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vymCp8X8IHEk",
        "colab_type": "code",
        "outputId": "71e6be37-59c8-4f11-c086-eb0564d60a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# dependencies\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# download nltk stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "zmVds2371DXz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_parsed_data(url):\n",
        "  return pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep='\\t', header=None)\n",
        "\n",
        "def get_parsed_data2(url):\n",
        "  return pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep=',', header='infer')\n",
        "\n",
        "\n",
        "columns_politifacts = [\n",
        "  'id',\n",
        "  'label',\n",
        "  'statement',\n",
        "  'subject',\n",
        "  'speaker',\n",
        "  'speakers_job_title',\n",
        "  'state_info',\n",
        "  'party_affiliation',\n",
        "  'barely_true',\n",
        "  'false',\n",
        "  'half_true',\n",
        "  'mostly_true',\n",
        "  'pants_on_fire',\n",
        "  'context',\n",
        "]\n",
        "\n",
        "\n",
        "# download and parse the dataset...\n",
        "data_poli_test  = get_parsed_data('https://raw.githubusercontent.com/synle/machine-learning-sample-dataset/master/liar_dataset/test.tsv')\n",
        "data_poli_train = get_parsed_data('https://raw.githubusercontent.com/synle/machine-learning-sample-dataset/master/liar_dataset/train.tsv')\n",
        "data_poli_valid = get_parsed_data('https://raw.githubusercontent.com/synle/machine-learning-sample-dataset/master/liar_dataset/valid.tsv')\n",
        "data_kg_fake_news = get_parsed_data2('https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle/kaggle-fake.csv')\n",
        "\n",
        "# parsed the columns\n",
        "data_poli_test.columns  = columns_politifacts\n",
        "data_poli_train.columns = columns_politifacts\n",
        "data_poli_valid.columns = columns_politifacts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3P1Fp1XF8TNi",
        "colab_type": "code",
        "outputId": "ff24df73-b867-425d-f6e4-5a83ae434088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "data_poli_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakers_job_title</th>\n",
              "      <th>state_info</th>\n",
              "      <th>party_affiliation</th>\n",
              "      <th>barely_true</th>\n",
              "      <th>false</th>\n",
              "      <th>half_true</th>\n",
              "      <th>mostly_true</th>\n",
              "      <th>pants_on_fire</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11972.json</td>\n",
              "      <td>true</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration</td>\n",
              "      <td>rick-perry</td>\n",
              "      <td>Governor</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>Radio interview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11685.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs</td>\n",
              "      <td>katrina-shankland</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a news conference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11096.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>military,veterans,voting-record</td>\n",
              "      <td>donald-trump</td>\n",
              "      <td>President-Elect</td>\n",
              "      <td>New York</td>\n",
              "      <td>republican</td>\n",
              "      <td>63</td>\n",
              "      <td>114</td>\n",
              "      <td>51</td>\n",
              "      <td>37</td>\n",
              "      <td>61</td>\n",
              "      <td>comments on ABC's This Week.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5209.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>rob-cornilles</td>\n",
              "      <td>consultant</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a radio show</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9524.json</td>\n",
              "      <td>pants-fire</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
              "      <td>state-democratic-party-wisconsin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>a web video</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id       label                                          statement  \\\n",
              "0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n",
              "1  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
              "2  11096.json       false  Says John McCain has done nothing to help the ...   \n",
              "3   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n",
              "4   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n",
              "\n",
              "                                             subject  \\\n",
              "0                                        immigration   \n",
              "1                                               jobs   \n",
              "2                    military,veterans,voting-record   \n",
              "3  medicare,message-machine-2012,campaign-adverti...   \n",
              "4  campaign-finance,legal-issues,campaign-adverti...   \n",
              "\n",
              "                            speaker    speakers_job_title state_info  \\\n",
              "0                        rick-perry              Governor      Texas   \n",
              "1                 katrina-shankland  State representative  Wisconsin   \n",
              "2                      donald-trump       President-Elect   New York   \n",
              "3                     rob-cornilles            consultant     Oregon   \n",
              "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
              "\n",
              "  party_affiliation  barely_true  false  half_true  mostly_true  \\\n",
              "0        republican           30     30         42           23   \n",
              "1          democrat            2      1          0            0   \n",
              "2        republican           63    114         51           37   \n",
              "3        republican            1      1          3            1   \n",
              "4          democrat            5      7          2            2   \n",
              "\n",
              "   pants_on_fire                       context  \n",
              "0             18               Radio interview  \n",
              "1              0             a news conference  \n",
              "2             61  comments on ABC's This Week.  \n",
              "3              1                  a radio show  \n",
              "4              7                   a web video  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "wflDgVH289N1",
        "colab_type": "code",
        "outputId": "6be76a5f-a57c-451b-c35e-2130878831c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "data_kg_fake_news.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>domain_rank</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2bdc29d12605ef9cf3f09f9875040a7113be5d5b</td>\n",
              "      <td>0</td>\n",
              "      <td>reasoning with facts</td>\n",
              "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
              "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
              "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c70e149fdd53de5e61c29281100b9de0ed268bc3</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
              "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
              "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7cf7c15731ac2a116dd7f629bd57ea468ed70284</td>\n",
              "      <td>0</td>\n",
              "      <td>Fed Up</td>\n",
              "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
              "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
              "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-01T15:46:26.304+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
              "      <td>0.068</td>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0206b54719c7e241ffe0ad4315b808290dbe6c0f</td>\n",
              "      <td>0</td>\n",
              "      <td>Fed Up</td>\n",
              "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
              "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
              "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-01T23:59:42.266+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
              "      <td>0.865</td>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       uuid  ord_in_thread  \\\n",
              "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
              "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
              "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
              "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
              "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
              "\n",
              "                 author                      published  \\\n",
              "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
              "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
              "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
              "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
              "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
              "\n",
              "                                               title  \\\n",
              "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
              "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
              "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
              "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
              "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
              "\n",
              "                                                text language  \\\n",
              "0  Print They should pay all the back all the mon...  english   \n",
              "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
              "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
              "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
              "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
              "\n",
              "                         crawled             site_url country  domain_rank  \\\n",
              "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
              "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
              "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
              "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
              "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
              "\n",
              "                                        thread_title  spam_score  \\\n",
              "0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n",
              "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
              "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
              "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
              "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
              "\n",
              "                                        main_img_url  replies_count  \\\n",
              "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
              "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
              "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
              "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
              "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
              "\n",
              "   participants_count  likes  comments  shares  type  \n",
              "0                   1      0         0       0  bias  \n",
              "1                   1      0         0       0  bias  \n",
              "2                   1      0         0       0  bias  \n",
              "3                   0      0         0       0  bias  \n",
              "4                   0      0         0       0  bias  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ZNEQFtevYBIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loop Helper for data wrangling\n",
        "def wrangle_text_loop(fun):\n",
        "  global data_poli_test\n",
        "  global data_poli_train\n",
        "  global data_poli_valid\n",
        "  global data_kg_fake_news\n",
        "  \n",
        "  data_poli_test['statement_clean_tokenize'] = data_poli_test['statement_clean_tokenize'].apply(lambda x: fun(x))\n",
        "  data_poli_train['statement_clean_tokenize'] = data_poli_train['statement_clean_tokenize'].apply(lambda x: fun(x))\n",
        "  data_poli_valid['statement_clean_tokenize'] = data_poli_valid['statement_clean_tokenize'].apply(lambda x: fun(x))\n",
        "  data_kg_fake_news['title_clean_tokenize'] = data_kg_fake_news['title_clean_tokenize'].apply(lambda x: fun(x))\n",
        "  \n",
        "  \n",
        "# this is the first setup  \n",
        "data_poli_test['statement_clean_tokenize'] = data_poli_test['statement']\n",
        "data_poli_train['statement_clean_tokenize'] = data_poli_train['statement']\n",
        "data_poli_valid['statement_clean_tokenize'] = data_poli_valid['statement']\n",
        "data_kg_fake_news['title_clean_tokenize'] = data_kg_fake_news['title']\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pixmO7kxnAiT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Identifying Factors</h2>\n",
        "\n",
        "| Factor | Rationale | Owner |\n",
        "| -- | -- | -- | \n",
        "| Context  | Type  of the medium (e.g. Large media outlet vs. small blog)  | sy|\n",
        "| Coverage of topic(s) near date published | Coverage of the latent event from outside sources | hyunwook |\n",
        "| Political Spectrum | number from 0 to 10 where 0 is left leaning and 10 is right leaning | hyunwook |\n",
        "| Sensationalism | Ranking from 0 to 10 with 10 being most sensational |  Mojdeh |\n",
        "| Social Reactions | Ranking from 0 to 10 with 10 being most active in the social interaction | sy|\n",
        "|Context Consistency | Ranking from 0 to 10 with 10 being most consistent in the topic and statements| Mojdeh |\n",
        "| WordToVec raw factors | Numeric vector representation of words in doc texts | Yu |\n",
        "|Text Raw factors|Raw factors(words) mesured by TF-IDF socre| Lin |"
      ]
    },
    {
      "metadata": {
        "id": "GkLr3Az4Ad8D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize The Strings"
      ]
    },
    {
      "metadata": {
        "id": "8kTMRyMd9nOB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokenize the word\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', str(text).lower())\n",
        "    return tokens\n",
        "  \n",
        "wrangle_text_loop(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMHmevQMAhRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords\n",
        "\n",
        "Remove words that are like `we`, `you`, etc..."
      ]
    },
    {
      "metadata": {
        "id": "fE6NUyyj_7rd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "from string import punctuation\n",
        "# english_stopwords = stopwords.words('english')\n",
        "english_stopwords = set(stopwords.words('english') + list(punctuation) + [''])\n",
        "\n",
        "\n",
        "def remove_stopwords(tokenized_words):\n",
        "  text = [word for word in tokenized_words if word not in english_stopwords]\n",
        "  return text\n",
        "\n",
        "\n",
        "wrangle_text_loop(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDxWICdKXlxc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatizing\n",
        "Simplify `meaning`, `meant`, `mean` into the same root word `mean`"
      ]
    },
    {
      "metadata": {
        "id": "WyKaQvB9Xbfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "def lemmatize(tokenized_words):\n",
        "  text = [wn.lemmatize(word) for word in tokenized_words]\n",
        "  return text\n",
        "\n",
        "\n",
        "ps = nltk.PorterStemmer()\n",
        "def stemming(tokenized_words):\n",
        "  text = [ps.stem(word) for word in tokenized_words]\n",
        "  return text\n",
        "\n",
        "wrangle_text_loop(stemming)\n",
        "wrangle_text_loop(lemmatize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "dd843027-b225-4da9-f1f1-35bfce01f69c",
        "id": "NT-mF8b2ap2t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "data_poli_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakers_job_title</th>\n",
              "      <th>state_info</th>\n",
              "      <th>party_affiliation</th>\n",
              "      <th>barely_true</th>\n",
              "      <th>false</th>\n",
              "      <th>half_true</th>\n",
              "      <th>mostly_true</th>\n",
              "      <th>pants_on_fire</th>\n",
              "      <th>context</th>\n",
              "      <th>statement_clean_tokenize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11972.json</td>\n",
              "      <td>true</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration</td>\n",
              "      <td>rick-perry</td>\n",
              "      <td>Governor</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>Radio interview</td>\n",
              "      <td>[, build, a, wall, on, the, u, s, mexico, bord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11685.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs</td>\n",
              "      <td>katrina-shankland</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a news conference</td>\n",
              "      <td>[, wisconsin, is, on, pace, to, doubl, the, nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11096.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>military,veterans,voting-record</td>\n",
              "      <td>donald-trump</td>\n",
              "      <td>President-Elect</td>\n",
              "      <td>New York</td>\n",
              "      <td>republican</td>\n",
              "      <td>63</td>\n",
              "      <td>114</td>\n",
              "      <td>51</td>\n",
              "      <td>37</td>\n",
              "      <td>61</td>\n",
              "      <td>comments on ABC's This Week.</td>\n",
              "      <td>[, say, john, mccain, ha, done, noth, to, help...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5209.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>rob-cornilles</td>\n",
              "      <td>consultant</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a radio show</td>\n",
              "      <td>[, suzann, bonamici, support, a, plan, that, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9524.json</td>\n",
              "      <td>pants-fire</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
              "      <td>state-democratic-party-wisconsin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>a web video</td>\n",
              "      <td>[, when, ask, by, a, report, whether, he, at, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id       label                                          statement  \\\n",
              "0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n",
              "1  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
              "2  11096.json       false  Says John McCain has done nothing to help the ...   \n",
              "3   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n",
              "4   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n",
              "\n",
              "                                             subject  \\\n",
              "0                                        immigration   \n",
              "1                                               jobs   \n",
              "2                    military,veterans,voting-record   \n",
              "3  medicare,message-machine-2012,campaign-adverti...   \n",
              "4  campaign-finance,legal-issues,campaign-adverti...   \n",
              "\n",
              "                            speaker    speakers_job_title state_info  \\\n",
              "0                        rick-perry              Governor      Texas   \n",
              "1                 katrina-shankland  State representative  Wisconsin   \n",
              "2                      donald-trump       President-Elect   New York   \n",
              "3                     rob-cornilles            consultant     Oregon   \n",
              "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
              "\n",
              "  party_affiliation  barely_true  false  half_true  mostly_true  \\\n",
              "0        republican           30     30         42           23   \n",
              "1          democrat            2      1          0            0   \n",
              "2        republican           63    114         51           37   \n",
              "3        republican            1      1          3            1   \n",
              "4          democrat            5      7          2            2   \n",
              "\n",
              "   pants_on_fire                       context  \\\n",
              "0             18               Radio interview   \n",
              "1              0             a news conference   \n",
              "2             61  comments on ABC's This Week.   \n",
              "3              1                  a radio show   \n",
              "4              7                   a web video   \n",
              "\n",
              "                            statement_clean_tokenize  \n",
              "0  [, build, a, wall, on, the, u, s, mexico, bord...  \n",
              "1  [, wisconsin, is, on, pace, to, doubl, the, nu...  \n",
              "2  [, say, john, mccain, ha, done, noth, to, help...  \n",
              "3  [, suzann, bonamici, support, a, plan, that, w...  \n",
              "4  [, when, ask, by, a, report, whether, he, at, ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "BroifyIxJ5-y",
        "colab_type": "code",
        "outputId": "1ec6c86b-b56e-49ca-c85d-9e0248eb258f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# why '' still there even if '' should have been removed?\n",
        "data_poli_test.statement_clean_tokenize[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'build',\n",
              " 'a',\n",
              " 'wall',\n",
              " 'on',\n",
              " 'the',\n",
              " 'u',\n",
              " 's',\n",
              " 'mexico',\n",
              " 'border',\n",
              " 'will',\n",
              " 'take',\n",
              " 'liter',\n",
              " 'year',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "B7hfp9XdPS0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_poli_test['statement_clean_concat'] = data_poli_test.statement_clean_tokenize.map(lambda x: \" \".join(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uKkW10xTScH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words = english_stopwords, sublinear_tf=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ip1QQN4wUEBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calculate tf_idf for each entry\n",
        "tf_idf_cleaned_statement = vectorizer.fit_transform(data_poli_test['statement_clean_concat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pxngE4AkilOT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_scores = []\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "for i in range(len(data_poli_test)):\n",
        "  feature_index = tf_idf_cleaned_statement[i,:].nonzero()[1]\n",
        "  temp = zip(feature_index, [tf_idf_cleaned_statement[i, x] for x in feature_index])\n",
        "  tfidf_scores.extend([(feature_names[j], s) for (j, s) in temp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIcnCaZFmFke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "tfidf_scores.sort(key=operator.itemgetter(1), reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54r93g5Ym5nR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now we can see the tf-idf scores for the dataset from high to low. Pick the top 100. This ranking can help us to build fake news scoring models."
      ]
    },
    {
      "metadata": {
        "id": "Un36n9crmUR1",
        "colab_type": "code",
        "outputId": "43d0942b-45e7-41f7-d2f7-3b28bf7e738c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "tfidf_scores[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('redistrict', 1.0),\n",
              " ('debat', 1.0),\n",
              " ('grow', 0.772497278191798),\n",
              " ('earmark', 0.7687601047099663),\n",
              " ('swing', 0.7529560010431467),\n",
              " ('appoint', 0.7426905708880303),\n",
              " ('toll', 0.7331823895171675),\n",
              " ('offshor', 0.7286239569164302),\n",
              " ('core', 0.7188739267100904),\n",
              " ('socialist', 0.7128796422439648),\n",
              " ('exception', 0.7061378693568888),\n",
              " ('bush', 0.7028556892532549),\n",
              " ('implement', 0.699925913826752),\n",
              " ('know', 0.6957909609698617),\n",
              " ('common', 0.6951404732112894),\n",
              " ('drill', 0.6849139576672708),\n",
              " ('financ', 0.6807599255930651),\n",
              " ('road', 0.6800320460845184),\n",
              " ('unauthor', 0.6731452286939492),\n",
              " ('recess', 0.6696347630716403),\n",
              " ('millionair', 0.6693301064630943),\n",
              " ('root', 0.6680757056311563),\n",
              " ('sue', 0.6679381377770834),\n",
              " ('custodi', 0.6672170390364597),\n",
              " ('150', 0.6617108264285471),\n",
              " ('athlet', 0.6596624707706761),\n",
              " ('ethanol', 0.6594352004136094),\n",
              " ('frisk', 0.6584164194641098),\n",
              " ('pledg', 0.657512950684523),\n",
              " ('wind', 0.6569918495570503),\n",
              " ('cap', 0.6563506103143171),\n",
              " ('nato', 0.6544046856947819),\n",
              " ('scholarship', 0.6479804632819792),\n",
              " ('czar', 0.6454006004299029),\n",
              " ('romanov', 0.6454006004299029),\n",
              " ('uninsur', 0.6418781706252383),\n",
              " ('ban', 0.639537255682827),\n",
              " ('economi', 0.6350180746925742),\n",
              " ('church', 0.6328139404152396),\n",
              " ('tpp', 0.6323742807630263),\n",
              " ('properti', 0.6312082913791965),\n",
              " ('product', 0.6292814976195364),\n",
              " ('ive', 0.6243621361774695),\n",
              " ('37th', 0.6175433310579863),\n",
              " ('poor', 0.6172643112594963),\n",
              " ('sale', 0.6159490945706056),\n",
              " ('rubl', 0.6153509981667924),\n",
              " ('assad', 0.6138149323864172),\n",
              " ('wealthiest', 0.612711630049259),\n",
              " ('anim', 0.6114646142564049),\n",
              " ('reconstruct', 0.61141632545629),\n",
              " ('casino', 0.6096886238359623),\n",
              " ('marijuana', 0.6089638267551606),\n",
              " ('budget', 0.6088084214329124),\n",
              " ('30', 0.6072004154678647),\n",
              " ('men', 0.6012234191159258),\n",
              " ('conspiraci', 0.5994798688340999),\n",
              " ('theorist', 0.5994798688340999),\n",
              " ('propos', 0.5992437090416124),\n",
              " ('mccollum', 0.5990121687615596),\n",
              " ('ceil', 0.597065574657879),\n",
              " ('type', 0.5969920092641408),\n",
              " ('invad', 0.5933912195646901),\n",
              " ('lobster', 0.5909217723264351),\n",
              " ('could', 0.5908265272394817),\n",
              " ('trade', 0.5908097136421359),\n",
              " ('ahead', 0.5878870706526469),\n",
              " ('jennif', 0.5878143470224599),\n",
              " ('carrol', 0.5878143470224599),\n",
              " ('santorum', 0.5864029320785858),\n",
              " ('august', 0.5841607233525596),\n",
              " ('confidenti', 0.5824646154309407),\n",
              " ('crowd', 0.5822806132481965),\n",
              " ('hundr', 0.5818215942458301),\n",
              " ('held', 0.5817231957529712),\n",
              " ('weak', 0.5809463882360951),\n",
              " ('rail', 0.5798727895600491),\n",
              " ('2015', 0.5793353981057567),\n",
              " ('refineri', 0.5792502844959683),\n",
              " ('serv', 0.579219490413085),\n",
              " ('american', 0.5779470345417812),\n",
              " ('bailout', 0.5772412508139179),\n",
              " ('student', 0.576556381810351),\n",
              " ('strongest', 0.575980172889493),\n",
              " ('1913', 0.5746547585817676),\n",
              " ('pryorcut', 0.5744883130004246),\n",
              " ('enemi', 0.5743201439370909),\n",
              " ('spot', 0.5732595031253724),\n",
              " ('civil', 0.5723243042994516),\n",
              " ('doubl', 0.5701556828691572),\n",
              " ('panther', 0.5690225496115001),\n",
              " ('worker', 0.5684093150155093),\n",
              " ('gun', 0.5683912684282749),\n",
              " ('measur', 0.5681349584307643),\n",
              " ('power', 0.567924784698576),\n",
              " ('clearanc', 0.5676724603354764),\n",
              " ('system', 0.567307919696145),\n",
              " ('surviv', 0.5670023799884405),\n",
              " ('amnesti', 0.5668067233025649),\n",
              " ('eighth', 0.5661130511705509)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "dLiIFBh-jpD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove Punctuation "
      ]
    },
    {
      "metadata": {
        "id": "dhVv25upj4FV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(tokenized_words):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = [w.translate(table) for w in tokenized_words]\n",
        "    return text \n",
        "  \n",
        "wrangle_text_loop(remove_punctuation)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oSRSXPMSZSkU",
        "colab_type": "code",
        "outputId": "fbad2f9d-1ab7-4adf-c172-0eb13a03f398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "data_poli_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakers_job_title</th>\n",
              "      <th>state_info</th>\n",
              "      <th>party_affiliation</th>\n",
              "      <th>barely_true</th>\n",
              "      <th>false</th>\n",
              "      <th>half_true</th>\n",
              "      <th>mostly_true</th>\n",
              "      <th>pants_on_fire</th>\n",
              "      <th>context</th>\n",
              "      <th>statement_clean_tokenize</th>\n",
              "      <th>statement_clean_concat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11972.json</td>\n",
              "      <td>true</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration</td>\n",
              "      <td>rick-perry</td>\n",
              "      <td>Governor</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>Radio interview</td>\n",
              "      <td>[, build, a, wall, on, the, u, s, mexico, bord...</td>\n",
              "      <td>build a wall on the u s mexico border will ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11685.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs</td>\n",
              "      <td>katrina-shankland</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a news conference</td>\n",
              "      <td>[, wisconsin, is, on, pace, to, doubl, the, nu...</td>\n",
              "      <td>wisconsin is on pace to doubl the number of l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11096.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>military,veterans,voting-record</td>\n",
              "      <td>donald-trump</td>\n",
              "      <td>President-Elect</td>\n",
              "      <td>New York</td>\n",
              "      <td>republican</td>\n",
              "      <td>63</td>\n",
              "      <td>114</td>\n",
              "      <td>51</td>\n",
              "      <td>37</td>\n",
              "      <td>61</td>\n",
              "      <td>comments on ABC's This Week.</td>\n",
              "      <td>[, say, john, mccain, ha, done, noth, to, help...</td>\n",
              "      <td>say john mccain ha done noth to help the vet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5209.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>rob-cornilles</td>\n",
              "      <td>consultant</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a radio show</td>\n",
              "      <td>[, suzann, bonamici, support, a, plan, that, w...</td>\n",
              "      <td>suzann bonamici support a plan that will cut ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9524.json</td>\n",
              "      <td>pants-fire</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
              "      <td>state-democratic-party-wisconsin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>a web video</td>\n",
              "      <td>[, when, ask, by, a, report, whether, he, at, ...</td>\n",
              "      <td>when ask by a report whether he at the center...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id       label                                          statement  \\\n",
              "0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n",
              "1  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
              "2  11096.json       false  Says John McCain has done nothing to help the ...   \n",
              "3   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n",
              "4   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n",
              "\n",
              "                                             subject  \\\n",
              "0                                        immigration   \n",
              "1                                               jobs   \n",
              "2                    military,veterans,voting-record   \n",
              "3  medicare,message-machine-2012,campaign-adverti...   \n",
              "4  campaign-finance,legal-issues,campaign-adverti...   \n",
              "\n",
              "                            speaker    speakers_job_title state_info  \\\n",
              "0                        rick-perry              Governor      Texas   \n",
              "1                 katrina-shankland  State representative  Wisconsin   \n",
              "2                      donald-trump       President-Elect   New York   \n",
              "3                     rob-cornilles            consultant     Oregon   \n",
              "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
              "\n",
              "  party_affiliation  barely_true  false  half_true  mostly_true  \\\n",
              "0        republican           30     30         42           23   \n",
              "1          democrat            2      1          0            0   \n",
              "2        republican           63    114         51           37   \n",
              "3        republican            1      1          3            1   \n",
              "4          democrat            5      7          2            2   \n",
              "\n",
              "   pants_on_fire                       context  \\\n",
              "0             18               Radio interview   \n",
              "1              0             a news conference   \n",
              "2             61  comments on ABC's This Week.   \n",
              "3              1                  a radio show   \n",
              "4              7                   a web video   \n",
              "\n",
              "                            statement_clean_tokenize  \\\n",
              "0  [, build, a, wall, on, the, u, s, mexico, bord...   \n",
              "1  [, wisconsin, is, on, pace, to, doubl, the, nu...   \n",
              "2  [, say, john, mccain, ha, done, noth, to, help...   \n",
              "3  [, suzann, bonamici, support, a, plan, that, w...   \n",
              "4  [, when, ask, by, a, report, whether, he, at, ...   \n",
              "\n",
              "                              statement_clean_concat  \n",
              "0   build a wall on the u s mexico border will ta...  \n",
              "1   wisconsin is on pace to doubl the number of l...  \n",
              "2      say john mccain ha done noth to help the vet   \n",
              "3   suzann bonamici support a plan that will cut ...  \n",
              "4   when ask by a report whether he at the center...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "mcrS3qXNHMHU",
        "colab_type": "code",
        "outputId": "d60df811-2153-4482-cc2e-553899da009b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "cell_type": "code",
      "source": [
        "data_kg_fake_news.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>...</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "      <th>title_clean_tokenize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>...</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[, muslim, bust, they, stole, million, in, gov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2bdc29d12605ef9cf3f09f9875040a7113be5d5b</td>\n",
              "      <td>0</td>\n",
              "      <td>reasoning with facts</td>\n",
              "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
              "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
              "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>...</td>\n",
              "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[, re, whi, did, attorney, gener, loretta, lyn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c70e149fdd53de5e61c29281100b9de0ed268bc3</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
              "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
              "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>...</td>\n",
              "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[, break, weiner, cooper, with, fbi, on, hilla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7cf7c15731ac2a116dd7f629bd57ea468ed70284</td>\n",
              "      <td>0</td>\n",
              "      <td>Fed Up</td>\n",
              "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
              "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
              "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-01T15:46:26.304+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>...</td>\n",
              "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
              "      <td>0.068</td>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[, pin, drop, speech, by, father, of, daughter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0206b54719c7e241ffe0ad4315b808290dbe6c0f</td>\n",
              "      <td>0</td>\n",
              "      <td>Fed Up</td>\n",
              "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
              "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
              "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-01T23:59:42.266+02:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>...</td>\n",
              "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
              "      <td>0.865</td>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[, fantast, trump, s, 7, point, plan, to, refo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       uuid  ord_in_thread  \\\n",
              "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
              "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
              "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
              "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
              "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
              "\n",
              "                 author                      published  \\\n",
              "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
              "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
              "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
              "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
              "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
              "\n",
              "                                               title  \\\n",
              "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
              "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
              "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
              "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
              "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
              "\n",
              "                                                text language  \\\n",
              "0  Print They should pay all the back all the mon...  english   \n",
              "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
              "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
              "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
              "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
              "\n",
              "                         crawled             site_url country  \\\n",
              "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US   \n",
              "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US   \n",
              "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US   \n",
              "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US   \n",
              "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US   \n",
              "\n",
              "                         ...                          \\\n",
              "0                        ...                           \n",
              "1                        ...                           \n",
              "2                        ...                           \n",
              "3                        ...                           \n",
              "4                        ...                           \n",
              "\n",
              "                                        thread_title spam_score  \\\n",
              "0  Muslims BUSTED: They Stole Millions In Gov’t B...      0.000   \n",
              "1  Re: Why Did Attorney General Loretta Lynch Ple...      0.000   \n",
              "2  BREAKING: Weiner Cooperating With FBI On Hilla...      0.000   \n",
              "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...      0.068   \n",
              "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...      0.865   \n",
              "\n",
              "                                        main_img_url replies_count  \\\n",
              "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...             0   \n",
              "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...             0   \n",
              "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...             0   \n",
              "3  http://100percentfedup.com/wp-content/uploads/...             0   \n",
              "4  http://100percentfedup.com/wp-content/uploads/...             0   \n",
              "\n",
              "   participants_count  likes  comments  shares  type  \\\n",
              "0                   1      0         0       0  bias   \n",
              "1                   1      0         0       0  bias   \n",
              "2                   1      0         0       0  bias   \n",
              "3                   0      0         0       0  bias   \n",
              "4                   0      0         0       0  bias   \n",
              "\n",
              "                                title_clean_tokenize  \n",
              "0  [, muslim, bust, they, stole, million, in, gov...  \n",
              "1  [, re, whi, did, attorney, gener, loretta, lyn...  \n",
              "2  [, break, weiner, cooper, with, fbi, on, hilla...  \n",
              "3  [, pin, drop, speech, by, father, of, daughter...  \n",
              "4  [, fantast, trump, s, 7, point, plan, to, refo...  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "ahQ0uSHPMwXs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Apply ngrams "
      ]
    },
    {
      "metadata": {
        "id": "cfw5lv-rITw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bigrams \n",
        "from nltk import bigrams\n",
        "from nltk.util import ngrams\n",
        "def apply_ngrams(tokenized_words):\n",
        "  bigrams_list =  list (ngrams(tokenized_words,2))\n",
        "  return bigrams_list\n",
        "\n",
        "wrangle_text_loop(apply_ngrams)\n",
        "#data_poli_test.head()\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQujtmPkxvol",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TfidfVectorizer/SVD/Logistic Regression\n",
        "\n",
        "Now try to use TfidfVectorizer to get a matrix for further classification. To make it easier, we make the clssification binary. We also try applying SVD for dimension reduction."
      ]
    },
    {
      "metadata": {
        "id": "sq5Yu8r9CuLP",
        "colab_type": "code",
        "outputId": "53eaf222-cdeb-4cff-f1da-68fecbc51a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from string import punctuation\n",
        "from nltk import PorterStemmer\n",
        "import copy \n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "cachedStopWords = set(stopwords.words('english') + list(punctuation) + [''])\n",
        "\n",
        "print(data_poli_train.groupby(['label'])['label'].count())\n",
        "\n",
        "data_poli_train_b = copy.deepcopy(data_poli_train);\n",
        "data_poli_test_b = copy.deepcopy(data_poli_test);\n",
        "data_poli_train_b.loc[data_poli_train_b['label'].isin(['barely-true','half-true', 'pants-fire']), 'label'] = 'false'\n",
        "data_poli_train_b.loc[data_poli_train_b['label'].isin(['mostly-true','true']), 'label'] = 'true'\n",
        "data_poli_test_b.loc[data_poli_test_b['label'].isin(['barely-true','half-true', 'pants-fire']), 'label'] = 'false'\n",
        "data_poli_test_b.loc[data_poli_test_b['label'].isin(['mostly-true','true']), 'label'] = 'true'\n",
        "\n",
        "print(data_poli_train_b.groupby(['label'])['label'].count())\n",
        "\n",
        "def tokenize2(text):\n",
        "\n",
        "    min_length = 3\n",
        "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
        "    words = [word for word in words if word not in cachedStopWords]\n",
        "    tokens = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
        "    p = re.compile('[a-zA-Z]+')\n",
        "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token) >= min_length, tokens))\n",
        "    return filtered_tokens\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize2)\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=200,       \n",
        "                         algorithm='randomized',\n",
        "                         n_iter=10)\n",
        "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
        "                            ('svd', svd_model)])\n",
        "# svd_transformer=vectorizer\n",
        "    \n",
        "print((data_poli_train_b[\"statement\"]).shape)\n",
        "\n",
        "vectorised_train_documents = svd_transformer.fit_transform(data_poli_train_b[\"statement\"])\n",
        "print (vectorised_train_documents.shape)\n",
        "\n",
        "train_labels = data_poli_train_b[\"label\"]\n",
        "print (train_labels.shape)\n",
        "\n",
        "vectorised_test_documents = svd_transformer.transform(data_poli_test_b[\"statement\"])\n",
        "test_labels = data_poli_test_b[\"label\"]\n",
        "\n",
        "print(data_poli_test_b.groupby(['label'])['label'].count())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "label\n",
            "barely-true    1654\n",
            "false          1995\n",
            "half-true      2114\n",
            "mostly-true    1962\n",
            "pants-fire      839\n",
            "true           1676\n",
            "Name: label, dtype: int64\n",
            "label\n",
            "false    6602\n",
            "true     3638\n",
            "Name: label, dtype: int64\n",
            "(10240,)\n",
            "(10240, 200)\n",
            "(10240,)\n",
            "label\n",
            "false    818\n",
            "true     449\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rr0CuLScyfwN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then apply LogisticRegression on the TF-IDF output:"
      ]
    },
    {
      "metadata": {
        "id": "phf_8wOnDVJE",
        "colab_type": "code",
        "outputId": "d98bcf12-aa9b-42b3-fbfd-a01086475e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1561
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import LabelEncoder, Imputer, MaxAbsScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "vectorised_train_documents = scaler.fit_transform(vectorised_train_documents)\n",
        "\n",
        "# model = svm.SVC();\n",
        "# parameters = {'kernel':('linear','poly','rbf','sigmoid')}\n",
        "# lr_gridCV = GridSearchCV(model, parameters, cv=5, scoring='accuracy')\n",
        "# lr_gridCV = lr_gridCV.fit(vectorised_train_documents, train_labels)\n",
        "\n",
        "# print(lr_gridCV.best_score_)\n",
        "# print(lr_gridCV.best_params_)\n",
        "# y_pred = lr_gridCV.predict(vectorised_test_documents)\n",
        "# print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
        "\n",
        "logistic = LogisticRegression(class_weight={\"false\":3,\"true\":5})\n",
        "C = [0.01, 0.1, 1, 10]\n",
        "penalty = ['l1','l2']\n",
        "\n",
        "param_grid = dict(C=C, penalty=penalty)\n",
        "gs = GridSearchCV(logistic, param_grid=param_grid, cv= 5, scoring='accuracy')\n",
        "\n",
        "gs.fit(vectorised_train_documents, train_labels)\n",
        "print(gs.best_params_)\n",
        "\n",
        "y_pred=gs.predict(vectorised_test_documents)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
        "print(pd.DataFrame(metrics.confusion_matrix(test_labels, y_pred),\n",
        "             index=[['actual', 'actual'], ['true', 'false']],\n",
        "             columns=[['predicted', 'predicted'], ['true', 'false']]))\n",
        "# print(pd.Series(\n",
        "#     gs.best_estimator_.coef_.T.ravel(),\n",
        "#     index=vectorizer.get_feature_names()\n",
        "# ).sort_values(ascending=False)[:20])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4e6a21effad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorised_train_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G06Hf4XZPWYW",
        "colab_type": "code",
        "outputId": "b5b07274-7ec2-4ce6-e7d3-f7962072eda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(data_kg_fake_news.shape)\n",
        "print(data_kg_fake_news.groupby(['type'])['type'].count())\n",
        "\n",
        "data_kg_fake_news_b=copy.deepcopy(data_kg_fake_news);\n",
        "data_kg_fake_news_b.loc[data_kg_fake_news_b['type']!='bs', 'type'] = 'non-bs'\n",
        "X=data_kg_fake_news_b['text']\n",
        "y=data_kg_fake_news_b['type']\n",
        "\n",
        "print(data_kg_fake_news_b.groupby(['type'])['type'].count())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize2)\n",
        "svd_model = TruncatedSVD(n_components=200,       \n",
        "                         algorithm='randomized',\n",
        "                         n_iter=10)\n",
        "# svd_transformer = Pipeline([('tfidf', vectorizer), \n",
        "#                             ('svd', svd_model)])\n",
        "svd_transformer=vectorizer\n",
        "vectorised_train_documents2 = svd_transformer.fit_transform(X_train.astype('U'))\n",
        "vectorised_test_documents2 = svd_transformer.transform(X_test.astype('U'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12999, 21)\n",
            "type\n",
            "bias            443\n",
            "bs            11492\n",
            "conspiracy      430\n",
            "fake             19\n",
            "hate            246\n",
            "junksci         102\n",
            "satire          146\n",
            "state           121\n",
            "Name: type, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e5dy8TJ4W_1T",
        "colab_type": "code",
        "outputId": "dc9ee621-eb95-47fb-fbed-1713bf639aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# logistic = LogisticRegression(class_weight={\"bs\":1,\"non-bs\":5})\n",
        "# C = [0.1, 1]\n",
        "# penalty = ['l2']\n",
        "\n",
        "# param_grid = dict(C=C, penalty=penalty)\n",
        "# gs = GridSearchCV(logistic, param_grid=param_grid, cv= 5, scoring='accuracy')\n",
        "# gs.fit(vectorised_train_documents2, y_train)\n",
        "\n",
        "gs=RandomForestClassifier()\n",
        "gs.fit(vectorised_train_documents2, y_train)\n",
        "print(vectorised_train_documents2.shape)\n",
        "feature_imp = pd.Series(gs.feature_importances_,index=list(vectorizer.get_feature_names())).sort_values(ascending=False).nlargest(20)\n",
        "print(feature_imp)\n",
        "\n",
        "y_pred=gs.predict(vectorised_test_documents2)\n",
        "print(y_test.value_counts(sort=False))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9099, 97177)\n",
            "nan           0.028139\n",
            "naturalnew    0.008137\n",
            "disqu         0.004675\n",
            "min           0.004579\n",
            "via           0.004064\n",
            "load          0.003847\n",
            "greenfield    0.003593\n",
            "trump         0.003074\n",
            "believ        0.003066\n",
            "sjw           0.003061\n",
            "infowar       0.002848\n",
            "women         0.002602\n",
            "afp           0.002393\n",
            "keyword       0.002185\n",
            "duke          0.002108\n",
            "patrick       0.002057\n",
            "becom         0.001851\n",
            "toward        0.001726\n",
            "part          0.001717\n",
            "easley        0.001685\n",
            "dtype: float64\n",
            "bs        3402\n",
            "non-bs     498\n",
            "Name: type, dtype: int64\n",
            "Accuracy: 0.8902564102564102\n",
            "[[3394    8]\n",
            " [ 420   78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4foht1p-4sxP",
        "colab_type": "code",
        "outputId": "14170c62-a4da-4bbd-d87a-696e9ce1d59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        }
      },
      "cell_type": "code",
      "source": [
        "data1 = data_kg_fake_news_b.loc[data_kg_fake_news_b['type']=='bs', :]\n",
        "for col in ['domain_rank','spam_score','replies_count','participants_count', 'likes', 'comments', 'shares', 'type']:\n",
        "  print(col, \":\", data1[col].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "domain_rank : [   nan 18533. 78393. 25842.  3734. 94243. 42160. 12993. 48959. 61606.\n",
            "  6659.  7587. 93026.   967. 22665. 10352. 87365.   486. 23950. 39980.\n",
            " 36674. 60570. 77628. 22680. 25709.   553. 48175. 35381. 32069. 40299.\n",
            " 40480. 12387. 70635. 51784. 56000. 78345. 60463. 17592. 67400. 65917.\n",
            " 82758. 68648. 32398. 79662. 53149. 26413. 30230. 23040. 21114. 68691.\n",
            " 98679. 54210. 43327. 27046. 23349. 10414. 61423. 91187. 12191. 75353.\n",
            " 17423. 22432. 44451. 57497. 28149.   538. 31016. 63847. 62759. 22891.\n",
            " 91245.  1616. 96853. 18896. 91878. 33221. 63072. 56487. 65975. 65060.\n",
            "  8968. 18369. 93150.  2182. 19375. 42575. 37614.  9645. 14958. 30890.\n",
            " 84363. 24453. 49688. 23405. 72287. 14170. 51246.  3921. 85288. 34478.\n",
            " 62087. 48413. 48717. 64783. 58305. 46814. 36845. 14716. 17366. 67959.\n",
            " 35234. 43665.  2435.]\n",
            "spam_score : [0.    0.038 0.061 0.018 0.007 0.002 0.009 0.141 0.004 0.001 0.003 0.005\n",
            " 0.04  0.01  0.995 0.131 0.013 0.928 0.043 0.334 0.008 0.05  0.101 0.028\n",
            " 0.105 0.71  0.842 0.094 0.194 0.762 0.012 0.052 0.059 0.109 0.048 0.11\n",
            " 0.034 0.294 0.006 0.299 0.022 0.966 0.017 0.181 0.062 0.077 0.59  0.024\n",
            " 0.689 0.822 1.    0.164 0.122 0.469 0.049 0.045 0.236 0.982 0.629 0.91\n",
            " 0.191 0.039 0.19  0.781 0.607 0.12  0.106 0.994 0.053 0.021 0.046 0.027\n",
            " 0.16  0.178 0.032 0.2   0.148 0.999 0.946 0.087 0.097 0.895 0.747 0.182\n",
            " 0.015 0.778 0.144 0.031 0.323 0.125 0.117 0.693 0.344 0.212 0.244 0.219\n",
            " 0.174 0.056 0.145 0.06  0.714 0.201 0.56  0.99  0.011 0.697 0.146 0.233\n",
            " 0.216 0.852 0.211 0.025 0.044 0.124 0.129 0.019 0.237 0.014 0.102 0.123\n",
            " 0.156 0.068 0.137 0.61  0.384 0.409 0.443 0.974 0.037 0.319 0.377 0.647\n",
            " 0.258 0.15  0.035 0.823 0.162 0.983 0.473 0.016 0.227 0.081 0.273 0.241\n",
            " 0.217 0.167 0.503 0.998 0.958 0.992 0.281 0.448 0.078 0.952 0.345 0.234\n",
            " 0.099 0.177 0.196 0.02  0.223 0.492 0.339 0.579 0.927 0.179 0.551 0.445\n",
            " 0.036 0.165 0.896 0.482 0.095 0.139 0.701 0.625 0.199 0.168 0.348 0.478\n",
            " 0.28  0.186 0.09  0.026 0.392 0.208 0.175 0.847 0.193 0.155 0.238 0.121\n",
            " 0.536 0.705 0.188 0.112 0.584 0.734 0.192 0.069 0.789 0.745 0.988 0.661\n",
            " 0.997 0.326 0.023 0.925 0.899 0.612 0.861 0.996 0.921 0.904 0.133 0.868\n",
            " 0.508 0.556 0.347 0.729 0.22  0.849 0.209 0.083 0.398 0.051 0.248 0.204\n",
            " 0.218 0.277 0.357 0.793 0.746 0.254 0.215 0.266 0.381 0.057 0.239 0.447\n",
            " 0.098 0.786 0.213 0.07  0.076 0.029 0.033 0.322 0.042 0.358 0.205 0.243\n",
            " 0.206 0.291 0.255 0.088 0.158 0.221 0.214 0.189 0.993 0.084 0.684 0.063\n",
            " 0.166 0.111 0.082 0.783 0.173 0.79  0.869 0.161 0.956 0.135 0.279 0.365\n",
            " 0.297 0.565 0.143 0.231 0.926 0.35  0.851 0.677 0.169 0.844 0.054 0.511\n",
            " 0.73  0.134 0.226 0.774 0.096 0.114 0.608 0.154 0.183 0.18  0.506 0.985\n",
            " 0.882 0.047 0.765 0.671 0.229 0.104 0.151 0.13  0.615 0.324 0.401 0.883\n",
            " 0.675 0.136 0.118 0.509 0.524 0.264 0.585 0.98  0.184 0.581 0.66  0.639\n",
            " 0.945 0.859 0.488 0.444 0.672 0.571 0.918 0.642 0.578 0.505 0.6   0.14\n",
            " 0.856 0.602 0.749 0.555 0.17  0.159 0.171 0.47  0.091 0.567 0.843 0.528\n",
            " 0.662 0.641 0.113 0.486 0.971 0.434 0.479 0.263 0.272 0.338 0.418 0.89\n",
            " 0.435 0.496 0.864 0.553 0.268 0.48  0.34  0.331 0.535 0.257 0.477 0.1\n",
            " 0.529 0.138 0.08  0.875 0.41  0.483 0.917 0.897 0.24  0.474 0.422 0.804\n",
            " 0.867 0.119 0.471 0.923 0.589 0.837 0.906 0.485 0.953 0.203 0.877 0.073\n",
            " 0.431 0.071 0.369 0.614 0.828 0.491 0.559 0.267 0.055 0.68  0.03  0.066\n",
            " 0.225 0.711 0.065 0.176 0.758 0.46  0.149 0.634 0.887 0.649 0.622 0.086\n",
            " 0.62  0.153 0.863 0.152 0.545 0.408 0.962 0.618 0.367 0.432 0.659 0.968\n",
            " 0.903 0.592 0.197 0.132 0.963 0.256 0.321 0.195 0.507 0.245 0.871 0.286\n",
            " 0.079 0.972 0.064 0.075 0.691 0.713 0.674 0.574 0.63  0.163 0.412 0.5\n",
            " 0.8   0.261 0.126 0.949 0.687 0.325 0.072 0.663 0.302 0.523]\n",
            "replies_count : [  5   6   2   8   9   0   1   3   7  13  29  30   4  20  12  21  18  25\n",
            "  10  23  17  98 100  15  61  24  22  54  28 307 308  40  88 309  11  14\n",
            "  37  51  52  55  16  43  26]\n",
            "participants_count : [  1  19   2   0   3   6   7   4   5  13  20  18  14   8  15  72  64  49\n",
            "  66  16  17  48  29 238  12 239  25  80  11 240  23  26  33  24   9  32]\n",
            "likes : [  0 416   3 258 856 967   2  12 828  95   1 188  16   9   7  10  31 512\n",
            " 235 539  49  62  56 167  27 609 682 119 903 904  14 185 474   4  19  60\n",
            "  72 483  93 635 820 880 411 305 386 269 893 168 211 261 472 130 822 272\n",
            " 320 160 799  71 204  97  41 175 136 247 406 974 466  20 197 806 569 988\n",
            " 379 112 775 873 469 230 473 610 248 498 614 959 517 434 382 297  65 552\n",
            " 544 985 189 199  58  98  21 104  34 982 252 518 132 207 949 716 278 917\n",
            "  38 791  68 454  53 182  15   5  45  69 147 179 265 121  81   8 155 383\n",
            " 122 165 442  86 149  57 322  32 102  61 194  59 161  23 281 284  50 375\n",
            " 225  25 117  67 114  74   6  47 340 745 877 900 363  99 431 942 357 555\n",
            "  17 676 490 666 821 654 597  29 943  30  18 860 623 564 777 793 912 273\n",
            " 172 607 280 333 714]\n",
            "comments : [ 0  1  8 17  6 26  3 30  7  2  4  9  5 20 65 12 10 13 15]\n",
            "shares : [  0 416   3 258 856 967   2  12 828  95   1 188  16   9   7  10  31 512\n",
            " 235 539  49  62  56 167  27 609 682 119 903 904  14 185 474   4  19  60\n",
            "  72 483  93 635 820 880 411 305 386 269 893 168 211 261 472 130 822 272\n",
            " 320 160 799  71 204  97  41 175 136 247 406 974 466  20 197 806 569 988\n",
            " 379 112 775 873 469 230 473 610 248 498 614 959 517 434 382 297  65 552\n",
            " 544 985 189 199  58  98  21 104  34 982 252 518 132 207 949 716 278 917\n",
            "  38 791  68 454  53 182  15   5  45  69 147 179 265 121  81   8 155 383\n",
            " 122 165 442  86 149  57 322  32 102  61 194  59 161  23 281 284  50 375\n",
            " 225  25 117  67 114  74   6  47 340 745 877 900 363  99 431 942 357 555\n",
            "  17 676 490 666 821 654 597  29 943  30  18 860 623 564 777 793 912 273\n",
            " 172 607 280 333 714]\n",
            "type : ['bs']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y1Oh8gAN4vL5",
        "colab_type": "code",
        "outputId": "0dfb414e-12bd-4ac9-87f0-ceede134cf5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "data1.text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59       United States Marine Field McConnell  Plum Cit...\n",
              "60       So ,you have Rothschild banksters and British ...\n",
              "61       Here is the problem . The USA constitution sta...\n",
              "62       There is plenty of proof the machines are rigg...\n",
              "63       Trump has an excuse now to audit any vote with...\n",
              "64       He has got to go after him , he is the one cau...\n",
              "65       He has got to go after him , he is the one cau...\n",
              "66       There is plenty of proof the machines are rigg...\n",
              "67       Trump has an excuse now to audit any vote with...\n",
              "68       He has got to go after him , he is the one cau...\n",
              "69       United States Marine Field McConnell  Plum Cit...\n",
              "70       Field is correct about the 8a companies and Tr...\n",
              "71       Georg Soros the good oil . http://mailstar.net...\n",
              "72       There is a lot more than meets the eye to this...\n",
              "73       They tick all the boxes , flying under the rad...\n",
              "74       Field is correct about the 8a companies and Tr...\n",
              "75       Georg Soros the good oil . http://mailstar.net...\n",
              "76       Field is correct about the 8a companies and Tr...\n",
              "77       Georg Soros the good oil . http://mailstar.net...\n",
              "78       Georg Soros the good oil . http://mailstar.net...\n",
              "79       \\nRoger Stone: Hillary Plans to Steal Election...\n",
              "80       ASS..WR.WB.SAYA pak alresky TKI BRUNAY DARUSAL...\n",
              "81       United States Marine Field McConnell  Plum Cit...\n",
              "82       I dont know guys , i must say that attack look...\n",
              "83       Its true you know the CATHOLICS ARE BEHIND EVE...\n",
              "84       \\n\\nMindblowing Reason Elites Fear Donald Trum...\n",
              "85                                              Brilliant!\n",
              "86       \\nEXCLUSIVE #Breaking FBI Reopens Investigatio...\n",
              "87       Good Day AD I would have to say that we are un...\n",
              "88       United States Marine Field McConnell  Plum Cit...\n",
              "                               ...                        \n",
              "12969                                     Stock \"On Fire!\"\n",
              "12970    more like a burning bag of shit on a doorstep....\n",
              "12971    Everyone loved the SpaceX video of spaceship g...\n",
              "12972                   non gaap has always been bullshit.\n",
              "12973            Generally Confusing Accounting Principles\n",
              "12974    All this non-GAAP activity should be disclosed...\n",
              "12975    Also, since when did we start using \"non-GAAP\"...\n",
              "12976    .. \"SEC in  recent months has raised concern t...\n",
              "12977    I'm sure they drastically changed accounting m...\n",
              "12978                           Tesla's on FIRE! \\noh wait\n",
              "12979    Musk's cash furnace is the posterchild for IB ...\n",
              "12980    Non-GAAP = SEC* FRAUD\\nNon-GAAP = SEC* SWINDLE...\n",
              "12981         Can we shoot this delusional fuck to Mars! ?\n",
              "12982    A dollar to be made and all I got was a quarte...\n",
              "12983    when i awoke this morning the Dow Jones Propag...\n",
              "12984    Then you beat me by a quarter because all I go...\n",
              "12985    And for the last two weeks, Gold is \"naturally...\n",
              "12986    I love that video clip. I think it is a male v...\n",
              "12987    A big selloff into the election = TRUMP 2016! ...\n",
              "12988    Technical explanations are out of favor, so he...\n",
              "12989    Nope its oil.  Thank you for my technical win ...\n",
              "12990    The weekly Wednesday Oil follies from 10:20 (s...\n",
              "12991    Chipotle.coli Mexican Grill weighing on restau...\n",
              "12992    Ah the old pump and dump.  The market should b...\n",
              "12993    A messaging app is worth $40 billion dollars i...\n",
              "12994    It DOES allow you to put a dog face on top of ...\n",
              "12995    Wait till you see what happens to the valuatio...\n",
              "12996    I'm waiting for the one that puts a pussy on m...\n",
              "12997    $4 Billion even after they are known to be kee...\n",
              "12998    of course - how else would they disceminate te...\n",
              "Name: text, Length: 11492, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "TKT2_pkp7E5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Political Spectrum</h2>\n",
        "\n",
        "<h2>Word2Vec</h2>\n",
        "(hshin)\n",
        "\n",
        "Create a word2vector model. This model will be useful identifying the political affiliations of the document. For each non-trivial words (after distiliation), we create a similarity score with the words that are commonly used by authors associated wiht one party or another (such as \"tax cuts\", \"spending\" with republican and \"bigotry\", \"impeach\" with republicans as an example). It is important to note that the words frequently chosen by one party may not necessarily be something they support. For example, authors to the right of the spectrum do not like spending. Hence,  it is more likely that they would talk about spending than the democrats.\n",
        "\n",
        "If the document associate closely with the dictionary of words with specific political affiliations you can set the poliyical affiliation score accordingly. The higher the similarity, the greater the weight. \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TyjMSP6_80UY",
        "colab_type": "code",
        "outputId": "efd13376-358c-4485-e5bb-cb7f932c15db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 956kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 13.7MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d9/5c892a943fede7db0c0d3bd653570208ffb3c163bc57c1a9354acd65c921/boto3-1.9.42-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 17.8MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.42 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ad/706263fda4a8c673fd58c1cf03160dfdcf093d6614130193d3ce12a81fad/botocore-1.12.42-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.8MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.42->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.42->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.42 botocore-1.12.42 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wsJtKUy87XhZ",
        "colab_type": "code",
        "outputId": "5c3c5fea-4e8a-4ccb-a5d3-cec0182c9194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "statements = data_poli_train['statement_clean_tokenize']\n",
        "statements.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [, say, the, annies, list, political, group, s...\n",
              "1    [, when, did, the, decline, of, coal, start, i...\n",
              "2    [, hillary, clinton, agrees, with, john, mccai...\n",
              "3    [, health, care, reform, legislation, is, like...\n",
              "4    [, the, economic, turnaround, started, at, the...\n",
              "Name: statement_clean_tokenize, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYCR7TVp9TzN",
        "colab_type": "code",
        "outputId": "b8306323-0d45-41c0-996a-399f06c99317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "m = Word2Vec( statements )\n",
        "\n",
        "m.similarity( \"plan\", \"cut\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.782147"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "zFW6SfZQJ78c",
        "colab_type": "code",
        "outputId": "da48799b-238e-4e68-cf27-2ac6fd7f9ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "m.similarity( \"job\", \"economy\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7834891"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "T1pmfs2_KP1k",
        "colab_type": "code",
        "outputId": "7e3663a1-5b4a-4d5d-d806-f9c4662a3318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "m.similarity( \"people\", \"vote\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6091816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "2xmBpCmbYz6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use pipe and gridsearch to find which n_components is the best for LDA and the following logistic regression\n",
        "pipe = Pipeline([('tfidf', vectorizer), \n",
        "                 ('svd', svd_model),\n",
        "                 ('scaler', scaler),\n",
        "                 ('logistic', logistic)\n",
        "                ])\n",
        "\n",
        "X_train_, y_train_, X_test_, y_test_ = data_poli_train_b[\"statement\"].tolist(),train_labels,data_poli_test_b[\"statement\"].tolist(), test_labels\n",
        "\n",
        "parameters = {'svd__n_components': [200,500,1000],'logistic__C':C}\n",
        "grid_tf_idf_ = GridSearchCV(pipe, param_grid=parameters, cv=5, scoring='accuracy')\n",
        "grid_tf_idf_.fit(X_train_, y_train_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5pfT0MX044I",
        "colab_type": "code",
        "outputId": "7eaf261a-a39a-4eb5-8229-6c2b9126f1d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-16 00:15:48--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-11-16 00:15:48--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.21MB/s    in 15m 59s \n",
            "\n",
            "2018-11-16 00:31:47 (878 KB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBTQbGR12iif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
        "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
        "           for line in lines}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AztWrPNl29hf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(next(iter(word2vec.values())))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w64G3hhr8J-E",
        "colab_type": "code",
        "outputId": "3b4afeca-ca2e-4948-eafd-3bb584a1c786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "etree_w2v = Pipeline([\n",
        "    (\"word2vec_vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
        "    (\"SVD\", TruncatedSVD(n_components = 200)),\n",
        "    (\"random_forest\", RandomForestClassifier())])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0a8fd82acaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m etree_w2v = Pipeline([\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
            "\u001b[0;32m<ipython-input-13-58d675b797ae>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, word2vec)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# if a text is empty we should return a vector of zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# with the same dimensionality as all the other vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "A1czVfzv8UAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}